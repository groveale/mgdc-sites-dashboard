{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%spark\r\n",
        "println(\"Application Id: \" + spark.sparkContext.applicationId )\r\n",
        "println(\"Application Name: \" + spark.sparkContext.appName)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpoolag",
              "session_id": "1",
              "statement_id": 37,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-09-01T14:14:56.8385495Z",
              "session_start_time": null,
              "execution_start_time": "2023-09-01T14:14:57.0406399Z",
              "execution_finish_time": "2023-09-01T14:15:01.1636547Z",
              "spark_jobs": null,
              "parent_msg_id": "b4772682-ce0f-42be-87d8-7b3ac1ff6e21"
            },
            "text/plain": "StatementMeta(sparkpoolag, 1, 37, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Application Id: application_1693565264111_0002\nApplication Name: scala-pg_sparkpoolag_1693571957\n"
          ]
        }
      ],
      "execution_count": 74,
      "metadata": {
        "microsoft": {
          "language": "scala"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val storageAccountName = \"mgdcag\" // replace with your blob name\r\n",
        "val storageContainerName = \"sites-pg\" //replace with your container name\r\n",
        "\r\n",
        "// Storage path\r\n",
        "val adls_path = f\"abfss://$storageContainerName@$storageAccountName.dfs.core.windows.net\"\r\n",
        "\r\n",
        "// Sites path\r\n",
        "val latestSitesPath = adls_path + s\"/latest/sites/\"\r\n",
        "\r\n",
        "\r\n",
        "spark.conf.set(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\", \"false\")\r\n",
        "// if MSI Access not granted for syanpse workspace to blob then you might need to use below commands to read creds and to set spark conf\r\n",
        "//spark.conf.set(s\"fs.azure.account.key.${storageAccountName}.blob.core.windows.net\",mssparkutils.credentials.getConnectionStringOrCreds(\"synapseworkspacename-WorkspaceDefaultStorage\"))\r\n",
        "//spark.conf.set(s\"fs.azure.account.key.${storageAccountName}.blob.core.windows.net\",mssparkutils.credentials.getConnectionStringOrCreds(\"LS_ADLSGen2\"))\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpoolag",
              "session_id": "1",
              "statement_id": 38,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-09-01T14:14:56.920772Z",
              "session_start_time": null,
              "execution_start_time": "2023-09-01T14:15:01.325472Z",
              "execution_finish_time": "2023-09-01T14:15:10.15493Z",
              "spark_jobs": null,
              "parent_msg_id": "5268b951-c83e-4b21-ba48-ab8ffedaa3de"
            },
            "text/plain": "StatementMeta(sparkpoolag, 1, 38, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "storageAccountName: String = mgdcag\nstorageContainerName: String = sites-pg\nadls_path: String = abfss://sites-pg@mgdcag.dfs.core.windows.net\nlatestSitesPath: String = abfss://sites-pg@mgdcag.dfs.core.windows.net/latest/sites/\n"
          ]
        }
      ],
      "execution_count": 75,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Blob Access details\r\n",
        "\r\n",
        "https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/microsoft-spark-utilities?pivots=programming-language-scala#configure-access-to-azure-blob-storage"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read the sites dataset\r\n",
        "This are the files created by the MGDC copy tool"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val sitesDF =\r\n",
        "    spark\r\n",
        "      .read\r\n",
        "      .format(\"json\")\r\n",
        "      .option(\"recursiveFileLookup\", \"false\")\r\n",
        "      .load(latestSitesPath)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpoolag",
              "session_id": "1",
              "statement_id": 39,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-09-01T14:14:57.0043914Z",
              "session_start_time": null,
              "execution_start_time": "2023-09-01T14:15:10.3253133Z",
              "execution_finish_time": "2023-09-01T14:15:14.5208674Z",
              "spark_jobs": null,
              "parent_msg_id": "db3968cf-c72c-4a35-87ac-e1d54aeb0938"
            },
            "text/plain": "StatementMeta(sparkpoolag, 1, 39, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sitesDF: org.apache.spark.sql.DataFrame = [BlockAccessFromUnmanagedDevices: boolean, BlockDownloadOfAllFilesOnUnmanagedDevices: boolean ... 27 more fields]\n"
          ]
        }
      ],
      "execution_count": 76,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check we have the sites"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "// This is the site Id of a known site in the tenant. Please update\r\n",
        "\r\n",
        "display(sitesDF.filter(\"Id == 'd2166072-e770-418c-8486-d121f14ce21d'\"))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpoolag",
              "session_id": "1",
              "statement_id": 40,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-09-01T14:14:57.139327Z",
              "session_start_time": null,
              "execution_start_time": "2023-09-01T14:15:14.6795355Z",
              "execution_finish_time": "2023-09-01T14:15:18.8267574Z",
              "spark_jobs": null,
              "parent_msg_id": "798838fb-34d5-40cb-997c-a4822dd10bf2"
            },
            "text/plain": "StatementMeta(sparkpoolag, 1, 40, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "f8049b66-6e97-4a9e-9262-5b1f0731075c",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": "SynapseWidget(Synapse.DataFrame, f8049b66-6e97-4a9e-9262-5b1f0731075c)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 77,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enrich the Data\r\n",
        "This is where we add value as SharePoint CSAs. The PG have provided the pizza base, now we need to add those toppings\r\n",
        "\r\n",
        "## Add coloumns\r\n",
        "### Using UDFs (User-Defined Functions):\r\n",
        "You can define custom UDFs and use them to create new columns based on your specific logic. Here's an example:\r\n",
        "\r\n",
        "```scala\r\n",
        "import org.apache.spark.sql.functions._\r\n",
        "import org.apache.spark.sql.types._\r\n",
        "\r\n",
        "// Define a custom UDF\r\n",
        "val myUDF = udf((arg1: String, arg2: Int) => {\r\n",
        "  // Your custom logic here\r\n",
        "  // Return the value for the new column\r\n",
        "}, StringType)\r\n",
        "\r\n",
        "// Assuming you have a DataFrame called \"df\"\r\n",
        "val dfWithNewColumn = df.withColumn(\"newColumnName\", myUDF($\"existingColumn1\", $\"existingColumn2\"))\r\n",
        "```\r\n",
        "\r\n",
        "We will use this to add a boolean coloumn for OneDrive sites "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import org.apache.spark.sql.functions._\r\n",
        "import org.apache.spark.sql.types._\r\n",
        "\r\n",
        "// returns true if site is OneDrive\r\n",
        "// Slighty different to the example above as I was getting scalla errors\r\n",
        "val isOneDrive = udf((siteUrl: String) => siteUrl.contains(\"-my.sharepoint.com\"))\r\n",
        "\r\n",
        "// Assuming you have a DataFrame called \"df\"\r\n",
        "val sitesDFOD = sitesDF.withColumn(\"OneDriveSite\", isOneDrive($\"Url\"))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpoolag",
              "session_id": "1",
              "statement_id": 41,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-09-01T14:14:57.334414Z",
              "session_start_time": null,
              "execution_start_time": "2023-09-01T14:15:19.0324319Z",
              "execution_finish_time": "2023-09-01T14:15:26.1668989Z",
              "spark_jobs": null,
              "parent_msg_id": "0c73281d-0424-48b7-9311-3dbc4a93e69f"
            },
            "text/plain": "StatementMeta(sparkpoolag, 1, 41, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import org.apache.spark.sql.functions._\nimport org.apache.spark.sql.types._\nisOneDrive: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($$$a5cddfc4633c5dd8aa603ddc4f9aad5$$$$w$$Lambda$7052/542511872@14df77eb,BooleanType,List(Some(class[value[0]: string])),Some(class[value[0]: boolean]),None,false,true)\nsitesDFOD: org.apache.spark.sql.DataFrame = [BlockAccessFromUnmanagedDevices: boolean, BlockDownloadOfAllFilesOnUnmanagedDevices: boolean ... 28 more fields]\n"
          ]
        }
      ],
      "execution_count": 78,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check our site\r\n",
        "Shoudl be True for OneDrive or False for SPO Sites"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "// This is the site Id of a known site in the tenant. Please update\r\n",
        "\r\n",
        "display(sitesDFOD.filter(\"Id == 'd2166072-e770-418c-8486-d121f14ce21d'\"))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpoolag",
              "session_id": "1",
              "statement_id": 42,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-09-01T14:14:57.4516398Z",
              "session_start_time": null,
              "execution_start_time": "2023-09-01T14:15:26.3333949Z",
              "execution_finish_time": "2023-09-01T14:15:30.4463875Z",
              "spark_jobs": null,
              "parent_msg_id": "d25b1592-125c-40cd-b9ad-71bee5324e27"
            },
            "text/plain": "StatementMeta(sparkpoolag, 1, 42, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "f447240a-7013-46c4-a510-d0faa9a36b18",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": "SynapseWidget(Synapse.DataFrame, f447240a-7013-46c4-a510-d0faa9a36b18)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 79,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can print for 20 items using"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import org.apache.spark.sql.DataFrame\r\n",
        "\r\n",
        "val oneDriveColoumn: DataFrame = sitesDFOD.select(\"OneDriveSite\", \"Url\")\r\n",
        "// using truncate = flase paramer to see full urls\r\n",
        "oneDriveColoumn.show(20, truncate = false)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpoolag",
              "session_id": "1",
              "statement_id": 43,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-09-01T14:14:57.6462605Z",
              "session_start_time": null,
              "execution_start_time": "2023-09-01T14:15:30.6520738Z",
              "execution_finish_time": "2023-09-01T14:15:36.0991803Z",
              "spark_jobs": null,
              "parent_msg_id": "80e5efda-2d27-4269-90dd-3334059ae9a1"
            },
            "text/plain": "StatementMeta(sparkpoolag, 1, 43, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+----------------------------------------------------------------------------+\n|OneDriveSite|Url                                                                         |\n+------------+----------------------------------------------------------------------------+\n|false       |https://groverale.sharepoint.com/sites/CarTest2                             |\n|false       |https://groverale.sharepoint.com/sites/dev-countries3                       |\n|false       |https://groverale.sharepoint.com/sites/collaboration                        |\n|false       |https://groverale.sharepoint.com/sites/archivedev                           |\n|false       |https://groverale.sharepoint.com/sites/migtest10                            |\n|false       |https://groverale.sharepoint.com/sites/AdoptionTeam                         |\n|false       |https://groverale.sharepoint.com/sites/AchiveHub                            |\n|false       |https://groverale.sharepoint.com/sites/testcoms                             |\n|false       |https://groverale.sharepoint.com/sites/dev-hub3                             |\n|false       |https://groverale.sharepoint.com/sites/hybridwork                           |\n|false       |https://groverale.sharepoint.com/sites/templates                            |\n|false       |https://groverale.sharepoint.com/sites/archDemo2                            |\n|false       |https://groverale.sharepoint.com/sites/permission-pg                        |\n|false       |https://groverale.sharepoint.com/sites/alex                                 |\n|true        |https://groverale-my.sharepoint.com/personal/aimee_groverale_onmicrosoft_com|\n|false       |https://groverale.sharepoint.com/sites/dev-hub                              |\n|false       |https://groverale.sharepoint.com/teams/SP2010-Test-Data-Home                |\n|false       |https://groverale.sharepoint.com/sites/demoportal                           |\n|false       |https://groverale.sharepoint.com/sites/AlexTeam                             |\n|false       |https://groverale.sharepoint.com/sites/adoptifyag                           |\n+------------+----------------------------------------------------------------------------+\nonly showing top 20 rows\n\nimport org.apache.spark.sql.DataFrame\noneDriveColoumn: org.apache.spark.sql.DataFrame = [OneDriveSite: boolean, Url: string]\n"
          ]
        }
      ],
      "execution_count": 80,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enrich DF with API data\r\n",
        "\r\n",
        "We want to call an API then append data to the DF based on the reponse. High level example below\r\n",
        "\r\n",
        "```scala\r\n",
        "import org.apache.spark.sql.DataFrame\r\n",
        "import org.apache.spark.sql.functions._\r\n",
        "import org.json4s._\r\n",
        "import org.json4s.jackson.JsonMethods._\r\n",
        "\r\n",
        "// Step 1: Make the API request and obtain JSON data (simulated)\r\n",
        "val apiResponseJson = \"\"\"{\"data1\": \"value1\", \"data2\": \"value2\"}\"\"\"\r\n",
        "\r\n",
        "// Step 2: Parse the JSON data\r\n",
        "val parsedJson = parse(apiResponseJson)\r\n",
        "\r\n",
        "// Assuming you have a DataFrame called \"df\"\r\n",
        "// Step 3: Iterate through the DataFrame and add columns based on JSON data\r\n",
        "val dfWithNewColumns: DataFrame = df\r\n",
        "  .withColumn(\"data1_from_API\", lit(parsedJson \\ \"data1\"))\r\n",
        "  .withColumn(\"data2_from_API\", lit(parsedJson \\ \"data2\"))\r\n",
        "\r\n",
        "dfWithNewColumns.show()\r\n",
        "\r\n",
        "```"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Call API from Scala\r\n",
        "The function below can be used to make API calls and return the repsonse as a string"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import org.apache.http.client.methods.HttpGet\r\n",
        "import org.apache.http.impl.client.HttpClients\r\n",
        "import org.apache.http.util.EntityUtils\r\n",
        "\r\n",
        "def makeAPICall(apiUrl: String): String = {\r\n",
        "  val httpClient = HttpClients.createDefault()\r\n",
        "  val httpGet = new HttpGet(apiUrl)\r\n",
        "\r\n",
        "  val response = httpClient.execute(httpGet)\r\n",
        "  val entity = response.getEntity\r\n",
        "  val responseJson = EntityUtils.toString(entity)\r\n",
        "\r\n",
        "  responseJson\r\n",
        "}\r\n",
        "\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpoolag",
              "session_id": "1",
              "statement_id": 44,
              "state": "finished",
              "livy_statement_state": "cancelled",
              "queued_time": "2023-09-01T14:14:57.738174Z",
              "session_start_time": null,
              "execution_start_time": "2023-09-01T14:15:36.2601303Z",
              "execution_finish_time": "2023-09-01T14:15:37.1500161Z",
              "spark_jobs": null,
              "parent_msg_id": "b67316c4-2ea1-4daf-9a0c-6d5cdee62285"
            },
            "text/plain": "StatementMeta(sparkpoolag, 1, 44, Finished, Cancelled)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 81,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will first call the last activity API to get details around file and site activity "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "// This is an Azure function that is hooked up to my tenant. Call it if you want :)\r\n",
        "val apiUrl = \"https://site-function-ag.azurewebsites.net/api/LastUserActivity?\"\r\n",
        "val apiResponseJson = makeAPICall(apiUrl)\r\n",
        "\r\n",
        "// Now you can parse apiResponseJson and process it as needed\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": null,
              "statement_id": null,
              "state": "cancelled",
              "livy_statement_state": null,
              "queued_time": "2023-09-01T14:14:57.8458252Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": "2023-09-01T14:15:36.7941937Z",
              "spark_jobs": null,
              "parent_msg_id": "9f6d2c55-390f-47f9-a5e2-1b1d9eaf75d7"
            },
            "text/plain": "StatementMeta(, , , Cancelled, )"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can create a DataFrame from a JSON string using the spark.read.json method with a provided RDD (Resilient Distributed Dataset) of strings\r\n",
        "\r\n",
        "With this DF we can select the coloumns we need and drop the rest and join with our main dataset"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import org.apache.spark.sql.DataFrame\r\n",
        "import org.apache.spark.sql.functions._\r\n",
        "import org.json4s._\r\n",
        "import org.json4s.jackson.JsonMethods._\r\n",
        "\r\n",
        "// Step 1: Above\r\n",
        "\r\n",
        "// Step 2: Parse the JSON data\r\n",
        "//val parsedJson = parse(apiResponseJson)\r\n",
        "\r\n",
        "val jsonRDD = spark.sparkContext.parallelize(Seq(apiResponseJson))\r\n",
        "\r\n",
        "// Load JSON data into a DataFrame without specifying the schema\r\n",
        "val jsonDF = spark.read.json(jsonRDD)\r\n",
        "    .withColumnRenamed(\"SiteId\", \"Id\") // Rename the \"SiteId\" column as we use to join\r\n",
        "    .select(\"Id\", \"lastActivityDate\", \"activeFileCount\", \"pageViewCount\")\r\n",
        "\r\n",
        "// Join with existing dataset\r\n",
        "val sitesDFODLA = sitesDFOD.join(jsonDF, \"Id\")\r\n",
        "\r\n",
        "\r\n",
        "//sitesDFODLA.show()\r\n",
        "display(sitesDFODLA.filter(\"Id == '9b88c7ff-6b3f-4df0-9f64-ec6ec52bbb54'\"))\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": null,
              "statement_id": null,
              "state": "cancelled",
              "livy_statement_state": null,
              "queued_time": "2023-09-01T14:14:57.9431273Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": "2023-09-01T14:15:36.7947647Z",
              "spark_jobs": null,
              "parent_msg_id": "a96a0b8f-578f-4582-9170-ad23f940a96c"
            },
            "text/plain": "StatementMeta(, , , Cancelled, )"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## I can datascience\r\n",
        "\r\n",
        "Next steo is to call the API for each item in the DF. \r\n",
        "\r\n",
        "To call an API endpoint for each item in a DataFrame and create new columns based on the responses, you can use a combination of Spark's DataFrame transformations and User-Defined Functions (UDFs). Here's a high-level approach to achieve this:\r\n",
        "\r\n",
        "1. Define a UDF that makes the API call, processes the response, and returns the desired result.\r\n",
        "\r\n",
        "2. Apply the UDF to your DataFrame to create the new columns based on the API responses.\r\n",
        "\r\n",
        "Here's a step-by-step guide:\r\n",
        "\r\n",
        "Assuming you have a DataFrame named df and you want to call an API for each row, and then create new columns based on the API \r\n",
        "\r\n",
        "```scala\r\n",
        "import org.apache.spark.sql.functions._\r\n",
        "import org.apache.spark.sql.DataFrame\r\n",
        "\r\n",
        "// Define the UDF to make API calls and process responses\r\n",
        "val callApiAndProcessResponse = udf((inputColumn1: String, inputColumn2: String) => {\r\n",
        "  // Perform the API call here, process the response, and return the result\r\n",
        "  // You can use libraries like HttpClient to make the API call\r\n",
        "\r\n",
        "  val apiUrl = s\"https://api.example.com/resource?param1=$inputColumn1&param2=$inputColumn2\"\r\n",
        "  // Make the API call and process the response\r\n",
        "  // Replace this with your actual API call logic and response processing\r\n",
        "\r\n",
        "  val apiResponse = makeApiCall(apiUrl) // Your API call function\r\n",
        "  val processedResult = processApiResponse(apiResponse) // Your response processing logic\r\n",
        "\r\n",
        "  processedResult // Return the result based on the API response\r\n",
        "}, StringType)\r\n",
        "\r\n",
        "// Apply the UDF to create new columns\r\n",
        "val dfWithApiResponses: DataFrame = df.withColumn(\r\n",
        "  \"NewColumn1\",\r\n",
        "  callApiAndProcessResponse(col(\"Column1\"), col(\"Column2\")),\r\n",
        "  \"NewColumn2\",\r\n",
        "  callApiAndProcessResponse(col(\"Column3\"), col(\"Column4\"))\r\n",
        "  // Add more columns and corresponding API calls as needed\r\n",
        ")\r\n",
        "\r\n",
        "dfWithApiResponses.show()\r\n",
        "```\r\n",
        "\r\n",
        "Whilst this would work and is using similar methods to ealier we don't want to call the API for each new coloumn. We want to make one API call per site.\r\n",
        "\r\n",
        "## A new method\r\n",
        "\r\n",
        "extract a list of Id values from your DataFrame and then iterate through that list to perform actions for each Id. Here's a general outline of how you can do this:\r\n",
        "\r\n",
        "Extract a list of Id values from your DataFrame.\r\n",
        "Iterate through the list of Id values.\r\n",
        "For each Id, perform the desired actions.\r\n",
        "\r\n",
        "```scala\r\n",
        "import org.apache.spark.sql.functions._\r\n",
        "\r\n",
        "// Assuming you have a DataFrame named \"df\" with a column \"Id\"\r\n",
        "val idList: Array[String] = df.select(\"Id\").distinct().collect().map(row => row.getString(0))\r\n",
        "\r\n",
        "// Iterate through the list of Id values\r\n",
        "for (id <- idList) {\r\n",
        "  // Perform actions for each Id\r\n",
        "  println(s\"Processing Id: $id\")\r\n",
        "\r\n",
        "  // You can call your API or perform other actions here\r\n",
        "}\r\n",
        "```\r\n",
        "This may not be the most effective, but for our usecase it makese sense. Speed is not a concern at this point.\r\n",
        "\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import org.apache.spark.sql.functions._\r\n",
        "\r\n",
        "// Assuming you have a DataFrame named \"df\" with a column \"Id\"\r\n",
        "val idList: Array[String] = sitesDFODLA.select(\"Id\").distinct().collect().map(row => row.getString(0))\r\n",
        "\r\n",
        "\r\n",
        "// Pop the first item from the list to use as the schema\r\n",
        "val firstItem = idList.head\r\n",
        "//val remainingItems = idList.tail\r\n",
        "val remainingItems = idList.tail.take(5) // using 5 as we are in dev - don't want to call the API 100s\r\n",
        "\r\n",
        "// This is another Azure function that is hooked up to my tenant. Call it if you want :)\r\n",
        "val baseApiUrl = \"https://site-function-ag.azurewebsites.net/api/GetAdditionalSiteInfo\"\r\n",
        "// Use the makeAPICall function defined above\r\n",
        "val firstApiResponse = makeAPICall(s\"$baseApiUrl?siteId=$firstItem\")\r\n",
        "\r\n",
        "// Create a DataFrame based on the schema of the first item - Not suing as want to be dynaic so can update source\r\n",
        "//val schema = StructType(firstApiResponse.keys.map(fieldName => StructField(fieldName, StringType, nullable = false)).toSeq)\r\n",
        "//var apiResponseDF = spark.createDataFrame(Seq(Row.fromSeq(firstApiResponse.values.map(_.toString).toSeq)), schema)\r\n",
        "\r\n",
        "println(firstApiResponse)\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": null,
              "statement_id": null,
              "state": "cancelled",
              "livy_statement_state": null,
              "queued_time": "2023-09-01T14:14:58.0460071Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": "2023-09-01T14:15:36.7952208Z",
              "spark_jobs": null,
              "parent_msg_id": "63218777-db97-4b07-bc4c-4e22bb1d60d3"
            },
            "text/plain": "StatementMeta(, , , Cancelled, )"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "// Create an RDD from the data (Same as earlier)\r\n",
        "val rdd = spark.sparkContext.parallelize(Seq(firstApiResponse))\r\n",
        "\r\n",
        "// Create a DataFrame without specifying a schema (schema will be inferred)\r\n",
        "// creating as var (mutable) so that you can update it within the loop.\r\n",
        "var apiResponseDF = spark.read.json(rdd)\r\n",
        "\r\n",
        "// Iterate through the list of Id values\r\n",
        "for (id <- remainingItems) {\r\n",
        "    // Perform actions for each Id\r\n",
        "    println(s\"Processing Id: $id\")\r\n",
        "\r\n",
        "    // You can call your API or perform other actions here\r\n",
        "    val apiResponseJson = makeAPICall(s\"$baseApiUrl?siteId=$id\")\r\n",
        "\r\n",
        "    // Create a new row with ApiResponse and append it to the DataFrame\r\n",
        "    val newRowRDD = spark.sparkContext.parallelize(Seq(apiResponseJson))\r\n",
        "    // Create a DataFrame from the new row\r\n",
        "    val newRowDF = spark.read.json(newRowRDD)\r\n",
        "    apiResponseDF = apiResponseDF.union(newRowDF)\r\n",
        "\r\n",
        "}\r\n",
        "\r\n",
        "// Show the DataFrame with API responses\r\n",
        "// apiResponseDF.show()\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpoolag",
              "session_id": "1",
              "statement_id": 45,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-09-01T14:15:40.5975712Z",
              "session_start_time": null,
              "execution_start_time": "2023-09-01T14:15:40.8035064Z",
              "execution_finish_time": "2023-09-01T14:15:51.7351799Z",
              "spark_jobs": null,
              "parent_msg_id": "5fa2ec61-4b4a-4eab-be5a-ec495dbb7306"
            },
            "text/plain": "StatementMeta(sparkpoolag, 1, 45, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing Id: 533fbbba-b95b-4d56-9fe8-da688e047709\nProcessing Id: d1a1ec05-7528-4768-af43-9b9593e7470f\nProcessing Id: 55c23d0d-382d-41a6-92cd-3483a8415cb2\nProcessing Id: 81beb81b-c4e3-4451-bd4c-0fbf8ab02e21\nProcessing Id: 77ff9639-8e5a-4c7a-ab29-07f8fa646bd8\nrdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[243] at parallelize at <console>:180\nwarning: one deprecation (since 2.2.0); for details, enable `:setting -deprecation' or `:replay -deprecation'\napiResponseDF: org.apache.spark.sql.DataFrame = [numberOfDrives: bigint, numberOfItemsInLists: bigint ... 5 more fields]\nwarning: one deprecation (since 2.2.0); for details, enable `:setting -deprecation' or `:replay -deprecation'\n"
          ]
        }
      ],
      "execution_count": 82,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val siteIdColumn: DataFrame = apiResponseDF.select(\"siteId\")\r\n",
        "siteIdColumn.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpoolag",
              "session_id": "1",
              "statement_id": 46,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-09-01T14:17:49.9164064Z",
              "session_start_time": null,
              "execution_start_time": "2023-09-01T14:17:50.084212Z",
              "execution_finish_time": "2023-09-01T14:17:55.6461409Z",
              "spark_jobs": null,
              "parent_msg_id": "01319a98-c640-4c2b-9cdd-6b9b0bd985ad"
            },
            "text/plain": "StatementMeta(sparkpoolag, 1, 46, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n|              siteId|\n+--------------------+\n|498a8cad-7133-4d5...|\n|533fbbba-b95b-4d5...|\n|d1a1ec05-7528-476...|\n|55c23d0d-382d-41a...|\n|81beb81b-c4e3-445...|\n|77ff9639-8e5a-4c7...|\n+--------------------+\n\nsiteIdColumn: org.apache.spark.sql.DataFrame = [siteId: string]\n"
          ]
        }
      ],
      "execution_count": 83,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Join back with the main dataset\r\n",
        "We kind of want to have one master dataset as it will make the PowerBI task easier.\r\n",
        "\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val sitesMoreDetails = apiResponseDF\r\n",
        "    .withColumnRenamed(\"SiteId\", \"Id\")\r\n",
        "\r\n",
        "// Join with existing dataset\r\n",
        "// val sitesDFODLAMORE = sitesDFODLA.join(apiResponseDF, \"Id\")\r\n",
        "\r\n",
        "// We are going to join backwards as we only have 6 items in debug - Prod would use the above\r\n",
        "val sitesDFODLAMORE = sitesMoreDetails.join(sitesDFODLA, \"Id\")\r\n",
        "\r\n",
        "val moreColoumns: DataFrame = sitesDFODLAMORE.select(\"Id\", \"OneDriveSite\", \"numberOfDrives\", \"storageUsedInDrives\", \"lastActivityDate\", \"StorageMetrics.TotalFileStreamSize\")\r\n",
        "// using truncate = flase paramer to see full urls\r\n",
        "moreColoumns.show(20, truncate = false)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpoolag",
              "session_id": "1",
              "statement_id": 57,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-09-01T15:09:51.9097434Z",
              "session_start_time": null,
              "execution_start_time": "2023-09-01T15:09:52.1377919Z",
              "execution_finish_time": "2023-09-01T15:10:05.0545214Z",
              "spark_jobs": null,
              "parent_msg_id": "3b5a695e-c7d9-48e0-b7a0-09963889d204"
            },
            "text/plain": "StatementMeta(sparkpoolag, 1, 57, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------+------------+--------------+-------------------+-------------------+-------------------+\n|Id                                  |OneDriveSite|numberOfDrives|storageUsedInDrives|lastActivityDate   |TotalFileStreamSize|\n+------------------------------------+------------+--------------+-------------------+-------------------+-------------------+\n|498a8cad-7133-4d55-b283-b0864d61a49a|false       |1             |1465376            |2023-07-31T00:00:00|0                  |\n|81beb81b-c4e3-4451-bd4c-0fbf8ab02e21|false       |1             |1998608            |2023-08-18T00:00:00|352031             |\n|533fbbba-b95b-4d56-9fe8-da688e047709|false       |4             |69281704           |2018-10-10T00:00:00|3312213            |\n|d1a1ec05-7528-4768-af43-9b9593e7470f|false       |2             |4253787586         |null               |2123799473         |\n|55c23d0d-382d-41a6-92cd-3483a8415cb2|false       |3             |108583185          |2020-06-17T00:00:00|8532817            |\n|77ff9639-8e5a-4c7a-ab29-07f8fa646bd8|false       |1             |1497654            |2023-01-27T00:00:00|29644              |\n+------------------------------------+------------+--------------+-------------------+-------------------+-------------------+\n\nsitesMoreDetails: org.apache.spark.sql.DataFrame = [numberOfDrives: bigint, numberOfItemsInLists: bigint ... 5 more fields]\nsitesDFODLAMORE: org.apache.spark.sql.DataFrame = [Id: string, numberOfDrives: bigint ... 37 more fields]\nmoreColoumns: org.apache.spark.sql.DataFrame = [Id: string, OneDriveSite: boolean ... 4 more fields]\n"
          ]
        }
      ],
      "execution_count": 94,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Big Value Data Points\r\n",
        "\r\n",
        "This is where the real magic happens. With the data in the DF it's possible to work out previous version storage. What an insight, and we haven't even itterated every object.\r\n",
        "\r\n",
        "In the MGDC sites data set we can make the following assumption\r\n",
        "\r\n",
        "`PreviousVersionSize = TotalSize - TotalFileStreamSize - MetadataSize`\r\n",
        "\r\n",
        "With the addional data we now have we can make a far better assumption\r\n",
        "\r\n",
        "`PreviousVersionSize = TotalSize - TotalFileStreamSize - MetadataSize - storageUsedPreservationHold`\r\n",
        "\r\n",
        "In an ideal world we will also have list size and perhpas even pages library size that we can remove. This is just one example of what we can do with just a few extra toppings to add to this maverlous MGDC flavoured Pizza.\r\n",
        "\r\n",
        "We will use one of the UDFs from the start\r\n",
        "\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import org.apache.spark.sql.functions._\r\n",
        "import org.apache.spark.sql.types._\r\n",
        "\r\n",
        "// returns true if site is OneDrive\r\n",
        "// Slighty different to the example above as I was getting scalla errors\r\n",
        "val previousVersionSize = udf((totalSize: BigInt, totalFileStreamSize: BigInt, metadataSize: BigInt, storageUsedPreservationHold: BigInt) => \r\n",
        "    totalSize - totalFileStreamSize - metadataSize - storageUsedPreservationHold\r\n",
        ")\r\n",
        "// Assuming you have a DataFrame called \"df\"\r\n",
        "// TotalSize - TotalFileStreamSize - MetadataSize - storageUsedPreservationHold\r\n",
        "val sitesDFODLAMOREPV = sitesDFODLAMORE\r\n",
        "    .withColumn(\"PreviousVersionSize\", previousVersionSize($\"StorageMetrics.TotalSize\", $\"StorageMetrics.TotalFileStreamSize\", $\"StorageMetrics.MetadataSize\", $\"storageUsedPreservationHold\"))\r\n",
        "\r\n",
        "val pvColoumns: DataFrame = sitesDFODLAMOREPV.select(\"Id\", \"OneDriveSite\", \"PreviousVersionSize\", \"lastActivityDate\")\r\n",
        "// using truncate = flase paramer to see full urls\r\n",
        "pvColoumns.show(20, truncate = false)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpoolag",
              "session_id": "1",
              "statement_id": 58,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-09-01T15:11:03.4585466Z",
              "session_start_time": null,
              "execution_start_time": "2023-09-01T15:11:03.6362522Z",
              "execution_finish_time": "2023-09-01T15:11:18.7429583Z",
              "spark_jobs": null,
              "parent_msg_id": "c9b2f437-25f0-4da2-b6af-1000c1bbfb99"
            },
            "text/plain": "StatementMeta(sparkpoolag, 1, 58, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------+------------+-------------------+-------------------+\n|Id                                  |OneDriveSite|PreviousVersionSize|lastActivityDate   |\n+------------------------------------+------------+-------------------+-------------------+\n|533fbbba-b95b-4d56-9fe8-da688e047709|false       |13189018           |2018-10-10T00:00:00|\n|d1a1ec05-7528-4768-af43-9b9593e7470f|false       |2390944            |null               |\n|55c23d0d-382d-41a6-92cd-3483a8415cb2|false       |26523382           |2020-06-17T00:00:00|\n|81beb81b-c4e3-4451-bd4c-0fbf8ab02e21|false       |1552909            |2023-08-18T00:00:00|\n|498a8cad-7133-4d55-b283-b0864d61a49a|false       |1413165            |2023-07-31T00:00:00|\n|77ff9639-8e5a-4c7a-ab29-07f8fa646bd8|false       |1407544            |2023-01-27T00:00:00|\n+------------------------------------+------------+-------------------+-------------------+\n\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.types._\npreviousVersionSize: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($$$c1c91dda193eef78668663dd30698de$$$$w$$Lambda$7234/1160164311@116c38ba,DecimalType(38,0),List(Some(class[value[0]: decimal(38,0)]), Some(class[value[0]: decimal(38,0)]), Some(class[value[0]: decimal(38,0)]), Some(class[value[0]: decimal(38,0)])),Some(class[value[0]: decimal(38,0)]),None,true,true)\nsitesDFODLAMOREPV: org.apache.spark.sql.DataFrame = [Id: string, numberOfDrives: bigint ... 38 more fields]\npvColoumns: org.apache.spark.sql.DataFrame = [Id: string, OneDriveSite: boolean ... 2 more fields]\nerror: error while loading Decimal, class file '/opt/spark/jars/spark-catalyst_2.12-3.3.1.5.2-92314920.jar(org/apache/spark/sql/types/Decimal.class)' is broken\n(class java.lang.RuntimeException/error reading Scala signature of Decimal.class: assertion failed:\n  Decimal$DecimalIsFractional\n     while compiling: <console>\n        during phase: globalPhase=terminal, enteringPhase=jvm\n     library version: version 2.12.15\n    compiler version: version 2.12.15\n  reconstructed args: -usejavacp -Yrepl-class-based -Yrepl-outdir /mnt/var/hadoop/tmp/nm-local-dir/usercache/trusted-service-user/appcache/application_1693565264111_0002/container_1693565264111_0002_01_000001/tmp/spark824099602607513057\n\n  last tree to typer: TypeTree(class Byte)\n       tree position: line 6 of <console>\n            tree tpe: Byte\n              symbol: (final abstract) class Byte in package scala\n   symbol definition: final abstract class Byte extends  (a ClassSymbol)\n      symbol package: scala\n       symbol owners: class Byte\n           call site: constructor $eval in object $eval in package $line1157\n\n== Source file context for tree position ==\n\n     3\n     4 object $eval {\n     5   lazy val $result = res208\n     6   lazy val $print: _root_.java.lang.String =  {\n     7     $iw\n     8\n     9 \"\" )\n"
          ]
        }
      ],
      "execution_count": 95,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write back to blob storage\r\n",
        "\r\n",
        "This can wait until next week"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "scala"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {
        "f8049b66-6e97-4a9e-9262-5b1f0731075c": {
          "type": "Synapse.DataFrame",
          "sync_state": {
            "table": {
              "rows": [
                {
                  "0": "false",
                  "1": "false",
                  "2": "false",
                  "3": "2015-11-24T15:19:51Z",
                  "4": "EUR",
                  "5": "00000000-0000-0000-0000-000000000000",
                  "6": "00000000-0000-0000-0000-000000000000",
                  "7": "Open",
                  "8": "d2166072-e770-418c-8486-d121f14ce21d",
                  "9": "false",
                  "10": "false",
                  "11": "false",
                  "12": "false",
                  "13": "2023-05-22T08:36:09Z",
                  "14": "Full",
                  "15": {
                    "AadObjectId": "e4238485-1d04-4afd-ad31-ea8cab673d94",
                    "Email": "alex@groverale.onmicrosoft.com",
                    "Name": "Alex Grover"
                  },
                  "16": "false",
                  "17": "true",
                  "18": {
                    "LastItemModifiedDate": "2023-08-25T09:15:06.000Z",
                    "Id": "b6b8c884-a572-4bc7-9380-a0dd5f1bd7cc",
                    "Title": "Alex Grover",
                    "WebTemplateId": 21,
                    "WebTemplate": "SPSPERS"
                  },
                  "19": "true",
                  "20": "false",
                  "21": "2023-08-25T00:00:00Z",
                  "22": {
                    "MetadataSize": -8703190,
                    "TotalFileCount": 101,
                    "TotalFileStreamSize": 7503530,
                    "TotalSize": 2100238
                  },
                  "23": "1099511627776",
                  "24": "1794882637",
                  "25": "None",
                  "26": "https://groverale-my.sharepoint.com/personal/alex_groverale_onmicrosoft_com",
                  "27": "1",
                  "28": "75e67881-b174-484b-9d30-c581c7ebc177"
                }
              ],
              "schema": [
                {
                  "key": "0",
                  "name": "BlockAccessFromUnmanagedDevices",
                  "type": "boolean"
                },
                {
                  "key": "1",
                  "name": "BlockDownloadOfAllFilesOnUnmanagedDevices",
                  "type": "boolean"
                },
                {
                  "key": "2",
                  "name": "BlockDownloadOfViewableFilesOnUnmanagedDevices",
                  "type": "boolean"
                },
                {
                  "key": "3",
                  "name": "CreatedTime",
                  "type": "string"
                },
                {
                  "key": "4",
                  "name": "GeoLocation",
                  "type": "string"
                },
                {
                  "key": "5",
                  "name": "GroupId",
                  "type": "string"
                },
                {
                  "key": "6",
                  "name": "HubSiteId",
                  "type": "string"
                },
                {
                  "key": "7",
                  "name": "IBMode",
                  "type": "string"
                },
                {
                  "key": "8",
                  "name": "Id",
                  "type": "string"
                },
                {
                  "key": "9",
                  "name": "IsHubSite",
                  "type": "boolean"
                },
                {
                  "key": "10",
                  "name": "IsInRecycleBin",
                  "type": "boolean"
                },
                {
                  "key": "11",
                  "name": "IsTeamsChannelSite",
                  "type": "boolean"
                },
                {
                  "key": "12",
                  "name": "IsTeamsConnectedSite",
                  "type": "boolean"
                },
                {
                  "key": "13",
                  "name": "LastSecurityModifiedDate",
                  "type": "string"
                },
                {
                  "key": "14",
                  "name": "Operation",
                  "type": "string"
                },
                {
                  "key": "15",
                  "name": "Owner",
                  "type": "StructType(StructField(AadObjectId,StringType,true),StructField(Email,StringType,true),StructField(Name,StringType,true))"
                },
                {
                  "key": "16",
                  "name": "ReadLocked",
                  "type": "boolean"
                },
                {
                  "key": "17",
                  "name": "ReadOnly",
                  "type": "boolean"
                },
                {
                  "key": "18",
                  "name": "RootWeb",
                  "type": "StructType(StructField(Id,StringType,true),StructField(LastItemModifiedDate,StringType,true),StructField(Title,StringType,true),StructField(WebTemplate,StringType,true),StructField(WebTemplateId,LongType,true))"
                },
                {
                  "key": "19",
                  "name": "ShareByEmailEnabled",
                  "type": "boolean"
                },
                {
                  "key": "20",
                  "name": "ShareByLinkEnabled",
                  "type": "boolean"
                },
                {
                  "key": "21",
                  "name": "SnapshotDate",
                  "type": "string"
                },
                {
                  "key": "22",
                  "name": "StorageMetrics",
                  "type": "StructType(StructField(MetadataSize,LongType,true),StructField(TotalFileCount,LongType,true),StructField(TotalFileStreamSize,LongType,true),StructField(TotalSize,LongType,true))"
                },
                {
                  "key": "23",
                  "name": "StorageQuota",
                  "type": "bigint"
                },
                {
                  "key": "24",
                  "name": "StorageUsed",
                  "type": "bigint"
                },
                {
                  "key": "25",
                  "name": "TeamsChannelType",
                  "type": "string"
                },
                {
                  "key": "26",
                  "name": "Url",
                  "type": "string"
                },
                {
                  "key": "27",
                  "name": "WebCount",
                  "type": "bigint"
                },
                {
                  "key": "28",
                  "name": "ptenant",
                  "type": "string"
                }
              ],
              "truncated": false
            },
            "isSummary": false,
            "language": "scala"
          },
          "persist_state": {
            "view": {
              "type": "details",
              "tableOptions": {},
              "chartOptions": {
                "chartType": "bar",
                "aggregationType": "sum",
                "categoryFieldKeys": [
                  "0"
                ],
                "seriesFieldKeys": [
                  "23"
                ],
                "isStacked": false
              }
            }
          }
        },
        "f447240a-7013-46c4-a510-d0faa9a36b18": {
          "type": "Synapse.DataFrame",
          "sync_state": {
            "table": {
              "rows": [
                {
                  "0": "false",
                  "1": "false",
                  "2": "false",
                  "3": "2015-11-24T15:19:51Z",
                  "4": "EUR",
                  "5": "00000000-0000-0000-0000-000000000000",
                  "6": "00000000-0000-0000-0000-000000000000",
                  "7": "Open",
                  "8": "d2166072-e770-418c-8486-d121f14ce21d",
                  "9": "false",
                  "10": "false",
                  "11": "false",
                  "12": "false",
                  "13": "2023-05-22T08:36:09Z",
                  "14": "Full",
                  "15": {
                    "AadObjectId": "e4238485-1d04-4afd-ad31-ea8cab673d94",
                    "Email": "alex@groverale.onmicrosoft.com",
                    "Name": "Alex Grover"
                  },
                  "16": "false",
                  "17": "true",
                  "18": {
                    "LastItemModifiedDate": "2023-08-25T09:15:06.000Z",
                    "Id": "b6b8c884-a572-4bc7-9380-a0dd5f1bd7cc",
                    "Title": "Alex Grover",
                    "WebTemplateId": 21,
                    "WebTemplate": "SPSPERS"
                  },
                  "19": "true",
                  "20": "false",
                  "21": "2023-08-25T00:00:00Z",
                  "22": {
                    "MetadataSize": -8703190,
                    "TotalFileCount": 101,
                    "TotalFileStreamSize": 7503530,
                    "TotalSize": 2100238
                  },
                  "23": "1099511627776",
                  "24": "1794882637",
                  "25": "None",
                  "26": "https://groverale-my.sharepoint.com/personal/alex_groverale_onmicrosoft_com",
                  "27": "1",
                  "28": "75e67881-b174-484b-9d30-c581c7ebc177",
                  "29": "true"
                }
              ],
              "schema": [
                {
                  "key": "0",
                  "name": "BlockAccessFromUnmanagedDevices",
                  "type": "boolean"
                },
                {
                  "key": "1",
                  "name": "BlockDownloadOfAllFilesOnUnmanagedDevices",
                  "type": "boolean"
                },
                {
                  "key": "2",
                  "name": "BlockDownloadOfViewableFilesOnUnmanagedDevices",
                  "type": "boolean"
                },
                {
                  "key": "3",
                  "name": "CreatedTime",
                  "type": "string"
                },
                {
                  "key": "4",
                  "name": "GeoLocation",
                  "type": "string"
                },
                {
                  "key": "5",
                  "name": "GroupId",
                  "type": "string"
                },
                {
                  "key": "6",
                  "name": "HubSiteId",
                  "type": "string"
                },
                {
                  "key": "7",
                  "name": "IBMode",
                  "type": "string"
                },
                {
                  "key": "8",
                  "name": "Id",
                  "type": "string"
                },
                {
                  "key": "9",
                  "name": "IsHubSite",
                  "type": "boolean"
                },
                {
                  "key": "10",
                  "name": "IsInRecycleBin",
                  "type": "boolean"
                },
                {
                  "key": "11",
                  "name": "IsTeamsChannelSite",
                  "type": "boolean"
                },
                {
                  "key": "12",
                  "name": "IsTeamsConnectedSite",
                  "type": "boolean"
                },
                {
                  "key": "13",
                  "name": "LastSecurityModifiedDate",
                  "type": "string"
                },
                {
                  "key": "14",
                  "name": "Operation",
                  "type": "string"
                },
                {
                  "key": "15",
                  "name": "Owner",
                  "type": "StructType(StructField(AadObjectId,StringType,true),StructField(Email,StringType,true),StructField(Name,StringType,true))"
                },
                {
                  "key": "16",
                  "name": "ReadLocked",
                  "type": "boolean"
                },
                {
                  "key": "17",
                  "name": "ReadOnly",
                  "type": "boolean"
                },
                {
                  "key": "18",
                  "name": "RootWeb",
                  "type": "StructType(StructField(Id,StringType,true),StructField(LastItemModifiedDate,StringType,true),StructField(Title,StringType,true),StructField(WebTemplate,StringType,true),StructField(WebTemplateId,LongType,true))"
                },
                {
                  "key": "19",
                  "name": "ShareByEmailEnabled",
                  "type": "boolean"
                },
                {
                  "key": "20",
                  "name": "ShareByLinkEnabled",
                  "type": "boolean"
                },
                {
                  "key": "21",
                  "name": "SnapshotDate",
                  "type": "string"
                },
                {
                  "key": "22",
                  "name": "StorageMetrics",
                  "type": "StructType(StructField(MetadataSize,LongType,true),StructField(TotalFileCount,LongType,true),StructField(TotalFileStreamSize,LongType,true),StructField(TotalSize,LongType,true))"
                },
                {
                  "key": "23",
                  "name": "StorageQuota",
                  "type": "bigint"
                },
                {
                  "key": "24",
                  "name": "StorageUsed",
                  "type": "bigint"
                },
                {
                  "key": "25",
                  "name": "TeamsChannelType",
                  "type": "string"
                },
                {
                  "key": "26",
                  "name": "Url",
                  "type": "string"
                },
                {
                  "key": "27",
                  "name": "WebCount",
                  "type": "bigint"
                },
                {
                  "key": "28",
                  "name": "ptenant",
                  "type": "string"
                },
                {
                  "key": "29",
                  "name": "OneDriveSite",
                  "type": "boolean"
                }
              ],
              "truncated": false
            },
            "isSummary": false,
            "language": "scala"
          },
          "persist_state": {
            "view": {
              "type": "details",
              "tableOptions": {},
              "chartOptions": {
                "chartType": "bar",
                "aggregationType": "sum",
                "categoryFieldKeys": [
                  "0"
                ],
                "seriesFieldKeys": [
                  "23"
                ],
                "isStacked": false
              }
            }
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}