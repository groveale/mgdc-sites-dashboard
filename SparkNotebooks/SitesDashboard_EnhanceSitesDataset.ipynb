{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "microsoft": {
          "language": "scala"
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": null,
              "execution_start_time": null,
              "livy_statement_state": null,
              "parent_msg_id": "b4f21307-1410-426f-bd21-da04904d9fbb",
              "queued_time": "2023-09-14T14:44:08.4327644Z",
              "session_id": null,
              "session_start_time": "2023-09-14T14:44:08.4959198Z",
              "spark_jobs": null,
              "spark_pool": null,
              "state": "session_starting",
              "statement_id": null
            },
            "text/plain": [
              "StatementMeta(, , , SessionStarting, )"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%%spark\n",
        "println(\"Application Id: \" + spark.sparkContext.applicationId )\n",
        "println(\"Application Name: \" + spark.sparkContext.appName)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Set up variables\n",
        "\n",
        "These will need to be changed for each enviroment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": null,
              "execution_start_time": null,
              "livy_statement_state": null,
              "parent_msg_id": "4a94a570-94d8-4cb3-8fe3-4254901ba930",
              "queued_time": "2023-09-14T14:44:08.5654134Z",
              "session_id": null,
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": null,
              "state": "waiting",
              "statement_id": null
            },
            "text/plain": [
              "StatementMeta(, , , Waiting, )"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "val storageAccountName = \"mgdcag\" // replace with your blob name\n",
        "val storageContainerName = \"sites-dashboard\" //replace with your container name\n",
        "\n",
        "// Storage path\n",
        "val adls_path = f\"abfss://$storageContainerName@$storageAccountName.dfs.core.windows.net\"\n",
        "\n",
        "// Sites path\n",
        "val latestSitesPath = adls_path + s\"/sites/latest\"\n",
        "\n",
        "\n",
        "spark.conf.set(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\", \"false\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Read the datasets into DFs (Data Frames)\n",
        "This are the files created by the MGDC copy tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": null,
              "execution_start_time": null,
              "livy_statement_state": null,
              "parent_msg_id": "89d3bb1a-f83d-4f66-98a3-023e22790fff",
              "queued_time": "2023-09-14T14:44:08.760641Z",
              "session_id": null,
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": null,
              "state": "waiting",
              "statement_id": null
            },
            "text/plain": [
              "StatementMeta(, , , Waiting, )"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "val sitesDF =\n",
        "    spark\n",
        "      .read\n",
        "      .format(\"json\")\n",
        "      .option(\"recursiveFileLookup\", \"false\")\n",
        "      .load(latestSitesPath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": null,
              "execution_start_time": null,
              "livy_statement_state": null,
              "parent_msg_id": "2c0adf95-761d-4e67-8701-1c386d0801a2",
              "queued_time": "2023-09-14T14:44:08.8782206Z",
              "session_id": null,
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": null,
              "state": "waiting",
              "statement_id": null
            },
            "text/plain": [
              "StatementMeta(, , , Waiting, )"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "val sitesCount = sitesDF.count()\n",
        "println(s\"The number of sites: $sitesCount\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Enrich the Data\n",
        "\n",
        "Pretty sure this is called feature engineering \n",
        "\n",
        "This is where we add value as SharePoint CSAs. The PG have provided the pizza base, now we need to add those toppings\n",
        "\n",
        "## Add coloumns\n",
        "### Using UDFs (User-Defined Functions):\n",
        "You can define custom UDFs and use them to create new columns based on your specific logic. \n",
        "\n",
        "We will use this to add a boolean coloumn for OneDrive sites "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": null,
              "execution_start_time": null,
              "livy_statement_state": null,
              "parent_msg_id": "2e1b52f7-3bab-4504-8237-43c7b3e19497",
              "queued_time": "2023-09-14T14:44:08.99894Z",
              "session_id": null,
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": null,
              "state": "waiting",
              "statement_id": null
            },
            "text/plain": [
              "StatementMeta(, , , Waiting, )"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import org.apache.spark.sql.functions._\n",
        "import org.apache.spark.sql.types._\n",
        "\n",
        "// returns true if site is OneDrive\n",
        "// Slighty different to the example above as I was getting scalla errors\n",
        "val isOneDrive = udf((siteUrl: String) => siteUrl.contains(\"-my.sharepoint.com\"))\n",
        "\n",
        "val sitesDFOD = sitesDF.withColumn(\"OneDriveSite\", isOneDrive($\"Url\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Enrich DF with API data\n",
        "\n",
        "We want to call an API then append data to the DF based on the reponse.\n",
        "\n",
        "The function below can be used to make API calls and return the repsonse as a string. This function is used futher down in the notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": null,
              "execution_start_time": null,
              "livy_statement_state": null,
              "parent_msg_id": "3f105e14-1d5b-4d14-9e9c-d6add060bbd0",
              "queued_time": "2023-09-14T14:44:09.129792Z",
              "session_id": null,
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": null,
              "state": "waiting",
              "statement_id": null
            },
            "text/plain": [
              "StatementMeta(, , , Waiting, )"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import org.apache.http.client.methods.HttpGet\n",
        "import org.apache.http.impl.client.HttpClients\n",
        "import org.apache.http.util.EntityUtils\n",
        "\n",
        "def makeAPICall(apiUrl: String): String = {\n",
        "  val httpClient = HttpClients.createDefault()\n",
        "  val httpGet = new HttpGet(apiUrl)\n",
        "\n",
        "  val response = httpClient.execute(httpGet)\n",
        "  val entity = response.getEntity\n",
        "  val responseJson = EntityUtils.toString(entity)\n",
        "\n",
        "  responseJson\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "We will first call the last activity API to get details around file and site activity\n",
        "\n",
        "There is a C# Azure function that contains a number of endpoints that this solution uses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": null,
              "execution_start_time": null,
              "livy_statement_state": null,
              "parent_msg_id": "9d78a585-1256-4616-8c33-afe407cc0e20",
              "queued_time": "2023-09-14T14:44:09.3668242Z",
              "session_id": null,
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": null,
              "state": "waiting",
              "statement_id": null
            },
            "text/plain": [
              "StatementMeta(, , , Waiting, )"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import org.apache.spark.sql.DataFrame\n",
        "import org.apache.spark.sql.functions._\n",
        "import org.json4s._\n",
        "import org.json4s.jackson.JsonMethods._\n",
        "\n",
        "// This is an Azure function that is hooked up to my tenant. Call it if you want :)\n",
        "val apiUrl = \"https://site-function-ag.azurewebsites.net/api/GetLastUserActivityForSites?timePeriod=D180\"\n",
        "\n",
        "// Call the API\n",
        "val apiResponseJson = makeAPICall(apiUrl)\n",
        "\n",
        "// Now you can parse apiResponseJson and process it\n",
        "val jsonRDD = spark.sparkContext.parallelize(Seq(apiResponseJson))\n",
        "\n",
        "// Load JSON data into a DataFrame without specifying the schema\n",
        "val jsonDF = spark.read.json(jsonRDD)\n",
        "    .withColumnRenamed(\"SiteId\", \"Id\") // Rename the \"SiteId\" column as we use to join\n",
        "    .select(\"Id\", \"lastActivityDate\", \"activeFileCount\", \"pageViewCount\", \"fileCount\") // using select as we don't want to add all coloumns\n",
        "\n",
        "// Join with existing dataset\n",
        "val sitesDFODLA = sitesDFOD.join(jsonDF, \"Id\")\n",
        "\n",
        "\n",
        "//sitesDFODLA.show()\n",
        "display(sitesDFODLA.filter(\"Id == '9b88c7ff-6b3f-4df0-9f64-ec6ec52bbb54'\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Next steo is to call the API for each item in the DF.\n",
        "\n",
        "We need to do this as we want to call an API for each site and get further details for each site and enrich with specific site data\n",
        "\n",
        "extract a list of Id values from your DataFrame and then iterate through that list to perform actions for each Id. Here's a general outline of how you can do this:\n",
        "\n",
        "Extract a list of Id values from your DataFrame.\n",
        "Iterate through the list of Id values.\n",
        "For each Id, perform the desired actions.\n",
        "\n",
        "```scala\n",
        "import org.apache.spark.sql.functions._\n",
        "\n",
        "// Assuming you have a DataFrame named \"df\" with a column \"Id\"\n",
        "val idList: Array[String] = df.select(\"Id\").distinct().collect().map(row => row.getString(0))\n",
        "\n",
        "// Iterate through the list of Id values\n",
        "for (id <- idList) {\n",
        "  // Perform actions for each Id\n",
        "  println(s\"Processing Id: $id\")\n",
        "\n",
        "  // You can call your API or perform other actions here\n",
        "}\n",
        "```\n",
        "This may not be the most effective, but for our usecase it makese sense. Speed is not a concern at this point.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## First Item to inliase the DFs\n",
        "There may be a way to do this without using the firs item to initlaise the DFs but I'm still learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": null,
              "execution_start_time": null,
              "livy_statement_state": null,
              "parent_msg_id": "2cc929ec-ddca-487c-bd66-4739b59e9f81",
              "queued_time": "2023-09-14T14:44:09.6917966Z",
              "session_id": null,
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": null,
              "state": "waiting",
              "statement_id": null
            },
            "text/plain": [
              "StatementMeta(, , , Waiting, )"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import org.apache.spark.sql.functions._\n",
        "\n",
        "// Assuming you have a DataFrame named \"df\" with a column \"Id\"\n",
        "val idList: Array[String] = sitesDFODLA.select(\"Id\").distinct().collect().map(row => row.getString(0))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": null,
              "execution_start_time": null,
              "livy_statement_state": null,
              "parent_msg_id": "c01eeac0-08c1-4244-9e40-d9e5440a9103",
              "queued_time": "2023-09-14T14:44:09.8961195Z",
              "session_id": null,
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": null,
              "state": "waiting",
              "statement_id": null
            },
            "text/plain": [
              "StatementMeta(, , , Waiting, )"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "// Pop the first item from the list to use as the schema\n",
        "val firstItem = idList.head\n",
        "\n",
        "// Perform actions for first Id\n",
        "println(s\"Processing Id: $firstItem\")\n",
        "\n",
        "var itemResult = sitesDFODLA.filter(s\"Id == '$firstItem'\").select(\"url\", \"Owner.AadObjectId\").collect()\n",
        "//var primaryAdminId = sitesDFODLA.filter(s\"Id == '$firstItem'\").select(\"Owner.AadObjectId\")\n",
        "\n",
        "var url = itemResult(0).getString(0)\n",
        "var primaryAdminId = itemResult(0).getString(1)\n",
        "\n",
        "val baseApiUrl = \"https://site-function-ag.azurewebsites.net/api/GetAdditionalSiteInfo\"\n",
        "\n",
        "var requestUrl = s\"$baseApiUrl?siteId=$firstItem&siteUrl=$url&primaryAdminId=$primaryAdminId\"   \n",
        "\n",
        "// You can call your API or perform other actions here\n",
        "val apiResponseJson = makeAPICall(s\"$requestUrl\")\n",
        "\n",
        "// Create a new row with ApiResponse and append it to the DataFrame\n",
        "val newRowRDD = spark.sparkContext.parallelize(Seq(apiResponseJson))\n",
        "val newRowDF = spark.read.json(newRowRDD)\n",
        "\n",
        "// Check if list column had Values\n",
        "if (newRowDF.select(\"Lists\").first().get(0) == null) {\n",
        "  println(\"No lists found\")\n",
        "} else {\n",
        "  println(\"Lists found\")\n",
        "}\n",
        "\n",
        "// Expload the lists\n",
        "val explodedDF = newRowDF.select(col(\"Lists\")).withColumn(\"exploded_data\", explode(col(\"Lists\")))\n",
        "\n",
        "// List DF (also mutalable)\n",
        "var listDF = explodedDF.select(col(\"*\"), col(\"exploded_data.*\")).drop(\"Lists\", \"exploded_data\")\n",
        "\n",
        "var apiResponseDF = newRowDF.drop(\"lists\")\n",
        "\n",
        "display(apiResponseDF)\n",
        "display(listDF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": null,
              "execution_start_time": null,
              "livy_statement_state": null,
              "parent_msg_id": "87767525-5b70-4358-99a0-5c83b50b93c2",
              "queued_time": "2023-09-14T14:44:10.114673Z",
              "session_id": null,
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": null,
              "state": "waiting",
              "statement_id": null
            },
            "text/plain": [
              "StatementMeta(, , , Waiting, )"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "val remainingItems = idList.tail\n",
        "//val remainingItems = idList.tail.take(5) // using 5 as we are in dev - don't want to call the API 100s\n",
        "\n",
        "// This is another Azure function that is hooked up to my tenant. Call it if you want :)\n",
        "val baseApiUrl = \"https://site-function-ag.azurewebsites.net/api/GetAdditionalSiteInfo\"\n",
        "\n",
        "for (id <- remainingItems) {\n",
        "    // Perform actions for each Id\n",
        "    println(s\"Processing Id: $id\")\n",
        "\n",
        "    var itemResult = sitesDFODLA.filter(s\"Id == '$id'\").select(\"url\", \"Owner.AadObjectId\").collect()\n",
        "    //var primaryAdminId = sitesDFODLA.filter(s\"Id == '$firstItem'\").select(\"Owner.AadObjectId\")\n",
        "\n",
        "    var url = itemResult(0).getString(0)\n",
        "    var primaryAdminId = itemResult(0).getString(1)\n",
        "\n",
        "    var requestUrl = s\"$baseApiUrl?siteId=$id&siteUrl=$url&primaryAdminId=$primaryAdminId\"   \n",
        "    \n",
        "    // You can call your API or perform other actions here\n",
        "    val apiResponseJson = makeAPICall(s\"$requestUrl\")\n",
        "\n",
        "    // Create a new row with ApiResponse and append it to the DataFrame\n",
        "    val newRowRDD = spark.sparkContext.parallelize(Seq(apiResponseJson))\n",
        "    // Create a DataFrame from the new row\n",
        "    val newRowDF = spark.read.json(newRowRDD)\n",
        "\n",
        "    // Expload the lists\n",
        "    val explodedDF = newRowDF.select(col(\"Lists\")).withColumn(\"exploded_data\", explode(col(\"Lists\")))\n",
        "    var listRowDF = explodedDF.select(col(\"*\"), col(\"exploded_data.*\")).drop(\"Lists\", \"exploded_data\")\n",
        "\n",
        "    // Add the Lists to the listDF (created in above cell)\n",
        "    listDF = listDF.union(listRowDF)\n",
        "\n",
        "    // Add the APIresponse but drop the lists\n",
        "    apiResponseDF = apiResponseDF.union(newRowDF.drop(\"Lists\"))\n",
        "\n",
        "}\n",
        "\n",
        "display(apiResponseDF)\n",
        "display(listDF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Join back with the main dataset\n",
        "We kind of want to have one master dataset as it will make the PowerBI task easier.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": null,
              "execution_start_time": null,
              "livy_statement_state": null,
              "parent_msg_id": "a9a7d543-bde7-4af7-a663-66e10f74e08e",
              "queued_time": "2023-09-14T14:44:10.2682762Z",
              "session_id": null,
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": null,
              "state": "waiting",
              "statement_id": null
            },
            "text/plain": [
              "StatementMeta(, , , Waiting, )"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "val sitesMoreDetails = apiResponseDF\n",
        "    .withColumnRenamed(\"SiteId\", \"Id\")\n",
        "\n",
        "// Join with existing dataset\n",
        "// val sitesDFODLAMORE = sitesDFODLA.join(apiResponseDF, \"Id\")\n",
        "\n",
        "// We are going to join backwards as we only have 6 items in debug - Prod would use the above\n",
        "val sitesDFODLAMORE = sitesMoreDetails.join(sitesDFODLA, \"Id\")\n",
        "\n",
        "display(sitesDFODLAMORE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Big Value Data Points\n",
        "\n",
        "This is where the real magic happens. With the data in the DF it's possible to work out previous version storage. What an insight, and we haven't even itterated every object.\n",
        "\n",
        "In the MGDC sites data set we can make the following assumption\n",
        "\n",
        "`PreviousVersionSize = TotalSize - TotalFileStreamSize - MetadataSize`\n",
        "\n",
        "With the addional data we now have we can make a far better assumption. We calcucate storage used in Drive by getting the size used by call the drives. We could probably even remove the metadata size\n",
        "\n",
        "`PreviousVersionSize = storageUsedInDrives - TotalFileStreamSize`\n",
        "\n",
        "This is just one example of what we can do with just a few extra toppings to add to this maverlous MGDC flavoured Pizza.\n",
        "\n",
        "We will use one of the UDFs from the start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": null,
              "execution_start_time": null,
              "livy_statement_state": null,
              "parent_msg_id": "c4061a84-1cb1-4f97-a77a-44faa09d012a",
              "queued_time": "2023-09-14T14:44:10.5625029Z",
              "session_id": null,
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": null,
              "state": "waiting",
              "statement_id": null
            },
            "text/plain": [
              "StatementMeta(, , , Waiting, )"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import org.apache.spark.sql.functions._\n",
        "import org.apache.spark.sql.types._\n",
        "\n",
        "// returns true if site is OneDrive\n",
        "// Slighty different to the example above as I was getting scalla errors\n",
        "val previousVersionSize = udf((storageUsedInDrives: BigInt, totalFileStreamSize: BigInt) => \n",
        "    storageUsedInDrives - totalFileStreamSize\n",
        ")\n",
        "// Assuming you have a DataFrame called \"df\"\n",
        "// TotalSize - TotalFileStreamSize - MetadataSize - storageUsedPreservationHold\n",
        "val sitesDFODLAMOREPV = sitesDFODLAMORE\n",
        "    .withColumn(\"PreviousVersionSize\", previousVersionSize($\"storageUsedInDrives\", $\"StorageMetrics.TotalFileStreamSize\"))\n",
        "\n",
        "val pvColoumns: DataFrame = sitesDFODLAMOREPV.select(\"Id\", \"OneDriveSite\", \"PreviousVersionSize\", \"lastActivityDate\")\n",
        "// using truncate = flase paramer to see full urls\n",
        "pvColoumns.show(20, truncate = false)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Write back to blob storage\n",
        "\n",
        "We need to write our new dataset back to the blobs - We will dropit in another location\n",
        "\n",
        "We also need to write out list DF from earlier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": null,
              "execution_start_time": null,
              "livy_statement_state": null,
              "parent_msg_id": "1159ca54-a9cd-42c9-85e5-4d96e1089765",
              "queued_time": "2023-09-14T14:44:10.7229544Z",
              "session_id": null,
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": null,
              "state": "waiting",
              "statement_id": null
            },
            "text/plain": [
              "StatementMeta(, , , Waiting, )"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "val latestSitesEnhanced = adls_path + s\"/sitesenhanced/latest/\"\n",
        "sitesDFODLAMOREPV\n",
        "    .repartition(1)\n",
        "    .write\n",
        "    .format(\"json\")\n",
        "    .mode(\"overwrite\")\n",
        "    .save(latestSitesEnhanced)\n",
        "\n",
        "\n",
        "val latestLists= adls_path + s\"/lists/latest/\"\n",
        "listDF\n",
        "    .repartition(1)\n",
        "    .write\n",
        "    .format(\"json\")\n",
        "    .mode(\"overwrite\")\n",
        "    .save(latestLists)"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "scala"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
